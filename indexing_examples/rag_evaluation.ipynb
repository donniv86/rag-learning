{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {},
 "cells": [
  {
   "id": "e6670728",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Install required packages (run this cell if you see import errors)\n!pip install numpy scikit-learn sentence-transformers llama-index-embeddings-openai llama-index-embeddings-huggingface\n",
   "outputs": []
  },
  {
   "id": "c200f4b8",
   "cell_type": "markdown",
   "source": "# RAG Evaluation and Metrics\n\nThis notebook explores various techniques for evaluating RAG systems.",
   "metadata": {}
  },
  {
   "id": "76bf9c3a",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "# Import required libraries\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score",
   "outputs": []
  },
  {
   "id": "13fde006",
   "cell_type": "markdown",
   "source": "## Exercise 1: Retrieval Metrics\n\nEvaluate the quality of document retrieval in RAG systems.",
   "metadata": {}
  },
  {
   "id": "bfbb04c0",
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "class RetrievalEvaluator:\n    def __init__(self, ground_truth):\n        self.ground_truth = ground_truth\n\n    def calculate_precision(self, retrieved_docs, k=5):\n        relevant_docs = set(self.ground_truth)\n        retrieved_docs = set(retrieved_docs[:k])\n\n        if not retrieved_docs:\n            return 0.0\n\n        return len(relevant_docs.intersection(retrieved_docs)) / len(retrieved_docs)\n\n    def calculate_recall(self, retrieved_docs, k=5):\n        relevant_docs = set(self.ground_truth)\n        retrieved_docs = set(retrieved_docs[:k])\n\n        if not relevant_docs:\n            return 0.0\n\n        return len(relevant_docs.intersection(retrieved_docs)) / len(relevant_docs)\n\n# Test retrieval metrics\nground_truth = ['doc1', 'doc2', 'doc3']\nretrieved_docs = ['doc1', 'doc4', 'doc2', 'doc5', 'doc3']\n\nevaluator = RetrievalEvaluator(ground_truth)\n\nprint(f\"Precision@5: {evaluator.calculate_precision(retrieved_docs):.3f}\")\nprint(f\"Recall@5: {evaluator.calculate_recall(retrieved_docs):.3f}\")",
   "outputs": []
  }
 ]
}